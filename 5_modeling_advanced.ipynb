{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, pandas as pd, seaborn as sns, warnings, numpy as np, ast\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set a random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_database = \"temp_gpt_4_1_nano_vecs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(path_database)\n",
    "display(database.head(3))\n",
    "database.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando setup de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_cols = [c for c in database.columns if \"embeddings\" in c]\n",
    "vec_names = [c.replace(\"embeddings_\", \"\") for c in emb_cols]\n",
    "\n",
    "# Convertendo strings de volta para listas\n",
    "for ec in emb_cols:\n",
    "    print(f\"Convertendo {ec} para lista...\")\n",
    "    database[ec] = [ast.literal_eval(x) for x in database[ec]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "PART = {}\n",
    "for c in database[\"company\"].drop_duplicates():\n",
    "    PART[c] = {}\n",
    "    for v in vec_names:\n",
    "        database_ = database[database[\"company\"] == c]\n",
    "\n",
    "        PART[c][v] = {\n",
    "            \"X\": list(database_[f\"embeddings_{v}\"]),\n",
    "            \"y\": list(database_[\"service_class\"])\n",
    "        }\n",
    "\n",
    "    print(f\"EMPRESA {i}: {c} - {vec_names} - {database_.shape}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_plot(c, df_model_metrics, save_file):\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # fig.suptitle(c.capitalize(), fontsize=16)\n",
    "\n",
    "    # Grade: 2 linhas, 2 colunas\n",
    "    gs = GridSpec(2, 2, height_ratios=[2, 1])\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    # === Paletas modernas ===\n",
    "    palette_main = [\n",
    "        \"#6366f1\", \"#10b981\", \"#f59e0b\", \"#ef4444\", \"#3b82f6\",\n",
    "        \"#8b5cf6\", \"#ec4899\", \"#14b8a6\", \"#f97316\", \"#0ea5e9\"\n",
    "    ]\n",
    "\n",
    "    palette_vec = [\n",
    "        \"#7dd3fc\", \"#93c5fd\", \"#c4b5fd\", \"#a5f3fc\"\n",
    "    ]\n",
    "\n",
    "    palette_model = [\n",
    "        \"#4ade80\", \"#facc15\", \"#fb7185\", \"#60a5fa\"\n",
    "    ]\n",
    "\n",
    "    # === Gráfico 1: Desempenho Geral ===\n",
    "    sns.barplot(\n",
    "        ax=ax1,\n",
    "        data=df_model_metrics,\n",
    "        x=\"model_name\", y=\"test_f1_macro\", hue=\"vec_name\",\n",
    "        errorbar=\"sd\", palette=palette_main, alpha=.85\n",
    "    )\n",
    "    ax1.set_title(\"Overall performance\")\n",
    "    ax1.set_ylabel(\"F1 Macro\")\n",
    "    ax1.set_xlabel(\"\")\n",
    "    ax1.legend(title=\"\", loc=\"upper right\")\n",
    "\n",
    "    # === Gráfico 2: Representação Vetorial ===\n",
    "    sns.barplot(\n",
    "        ax=ax2,\n",
    "        data=df_model_metrics,\n",
    "        x=\"vec_name\", y=\"test_f1_macro\",\n",
    "        errorbar=\"sd\", palette=palette_vec, alpha=.85\n",
    "    )\n",
    "    ax2.set_title(\"Vector representation\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.legend().remove()\n",
    "\n",
    "    # === Gráfico 3: Modelo ===\n",
    "    sns.barplot(\n",
    "        ax=ax3,\n",
    "        data=df_model_metrics,\n",
    "        x=\"model_name\", y=\"test_f1_macro\",\n",
    "        errorbar=\"sd\", palette=palette_model, alpha=.85\n",
    "    )\n",
    "    ax3.set_title(\"Model\")\n",
    "    ax3.set_ylabel(\"\")\n",
    "    ax3.set_xlabel(\"\")\n",
    "    ax3.legend().remove()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(save_file, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRADICIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=1000,                   # Mais árvores para estabilidade estatística e menor variância\n",
    "        max_depth=100,                       # Maior profundidade para capturar padrões complexos\n",
    "        max_features='log2',                 # Reduz ainda mais overfitting em dados textuais de alta dimensionalidade\n",
    "        min_samples_split=10,                # Evita splits prematuros em amostras pequenas\n",
    "        min_samples_leaf=4,                  # Evita folhas muito pequenas\n",
    "        bootstrap=True,                      # Mantém o bootstrap ativado para diversidade\n",
    "        class_weight='balanced_subsample',   # Compensa classes desbalanceadas durante bootstrap\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    \"SVM\": SVC(\n",
    "        C=1.0,                               # Regularização moderada. Pode testar 0.1, 1.0, 10 para tuning\n",
    "        kernel='rbf',                        # Kernel RBF captura padrões não lineares (recomendado para embeddings densos como BERT)\n",
    "        gamma='scale',                       # Define gamma automaticamente como 1 / n_features, robusto para começar\n",
    "        probability=True,                    # Permite predict_proba(), útil para ROC AUC ou thresholds probabilísticos\n",
    "        class_weight='balanced',             # Compensa classes desbalanceadas automaticamente\n",
    "        max_iter=10000,                      # Aumentado para evitar não convergir em datasets médios\n",
    "        random_state=42                      # Reprodutibilidade\n",
    "    ),\n",
    "\n",
    "    \"MLP\": MLPClassifier(\n",
    "        hidden_layer_sizes=(1024, 512, 128), # Arquitetura mais profunda, útil para capturar interações não-lineares complexas\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=1e-3,                          # Regularização mais forte (mais adequada para vetores textuais esparsos)\n",
    "        learning_rate='adaptive',\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,             # Mantém 10% para validação interna no early stopping\n",
    "        max_iter=500,                        # Permite mais épocas (mas com early stopping)\n",
    "        batch_size=32,                       # Reduzido para maior precisão gradiente em vetores pequenos\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_all = []\n",
    "for c in tqdm_notebook(PART.keys()):\n",
    "    model_metrics = []\n",
    "    for v in PART[c].keys():\n",
    "        for m in models.keys():\n",
    "            cv_results = cross_validate(\n",
    "                models[m], \n",
    "                PART[c][v][\"X\"], \n",
    "                PART[c][v][\"y\"], \n",
    "                cv=2,\n",
    "                scoring=('accuracy', 'precision', 'recall', 'f1_macro'),\n",
    "                return_train_score=True\n",
    "            )\n",
    "            \n",
    "            cvr = pd.DataFrame(cv_results)\n",
    "            cvr[\"company\"] = c\n",
    "            cvr[\"model_name\"] = m\n",
    "            cvr[\"vec_name\"] = v\n",
    "\n",
    "            mean = round(cvr['test_f1_macro'].mean(), 2)\n",
    "            std = round(cvr['test_f1_macro'].std(), 2)\n",
    "            print(f\"EMPRESA: {c}\\tVEC: {v}\\tMODEL: {m}\\tR: {mean}\\t+- {std}\")\n",
    "\n",
    "            model_metrics.append(cvr)\n",
    "\n",
    "    save_file = f\"./plot_results/{c}.png\".lower()\n",
    "    create_save_plot(c, model_metrics, save_file)\n",
    "\n",
    "    model_metrics_all += model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_metrics_all = pd.concat(model_metrics_all)\n",
    "df_model_metrics_all.to_excel(\"reclamacoes_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_cnn(df, save_file, n_epochs):\n",
    "    # Limpa qualquer figura anterior\n",
    "    plt.clf()  # limpa o conteúdo atual da figura\n",
    "    plt.close('all')  # fecha figuras abertas (opcional para liberar memória)\n",
    "\n",
    "    # Configurações do seaborn\n",
    "    sns.set(rc={'figure.figsize': (12, 9)})\n",
    "    sns.set_style(\"ticks\", {'axes.grid': True, 'grid.color': '.95'})\n",
    "    sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "    # Cria a figura e o eixo explicitamente\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Cria o gráfico\n",
    "    sns.lineplot(\n",
    "        x=\"Epochs\",\n",
    "        y=\"Variation\",\n",
    "        hue=\"Metrics\",\n",
    "        data=df,\n",
    "        ax=ax  # usa o eixo explicitamente para evitar global state\n",
    "    )\n",
    "\n",
    "    # Configura o gráfico\n",
    "    ax.legend(loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "    ax.set(xlim=(1, n_epochs))\n",
    "\n",
    "    # Salva o gráfico\n",
    "    plt.savefig(save_file, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Fecha a figura para garantir que nada fique em memória\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Configurações iniciais\n",
    "# =============================\n",
    "part_test = 0.1\n",
    "part_valid = 0.1\n",
    "n_epocas = 100\n",
    "repeticao_model = 2\n",
    "semente_model = True  # False: pesos aleatórios, True: sementes variando, int: fixa a semente\n",
    "\n",
    "for c in tqdm_notebook(PART.keys()):\n",
    "    model_metrics = []\n",
    "    for v in PART[c].keys():\n",
    "        # =============================\n",
    "        # Carregamento e pré-processamento\n",
    "        # =============================\n",
    "        X_all = np.array(PART[c][v][\"X\"])\n",
    "        Y_all = np.array(PART[c][v][\"y\"])\n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        Y_all_encoded = lb.fit_transform(Y_all)\n",
    "        if Y_all_encoded.shape[1] == 1:  # caso binário\n",
    "            Y_all_encoded = np.hstack((1 - Y_all_encoded, Y_all_encoded))\n",
    "\n",
    "        X_train_all, X_test, y_train_all, y_test = train_test_split(X_all, Y_all_encoded, test_size=part_test, random_state=13)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, test_size=part_valid, random_state=13)\n",
    "\n",
    "        # =============================\n",
    "        # Inicialização de métricas\n",
    "        # =============================\n",
    "        histories = []\n",
    "        predictions = []\n",
    "\n",
    "        # =============================\n",
    "        # Treinamento\n",
    "        # =============================\n",
    "        cv_results = {\n",
    "            'fit_time': [],\n",
    "            'score_time': [],\n",
    "            'test_accuracy': [],\n",
    "            'train_accuracy': [],\n",
    "            'test_precision': [],\n",
    "            'train_precision': [],\n",
    "            'test_recall': [],\n",
    "            'train_recall': [],\n",
    "            'test_f1_macro': [],\n",
    "            'train_f1_macro': []\n",
    "        }\n",
    "\n",
    "        for i in range(repeticao_model):\n",
    "            # Define semente\n",
    "            seed = i if semente_model is True else (semente_model if isinstance(semente_model, int) else None)\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "                print(f\"Semente = {seed}\")\n",
    "\n",
    "            # Otimizador\n",
    "            sgd = SGD(learning_rate=0.01, nesterov=True)\n",
    "\n",
    "            # Modelo\n",
    "            model = Sequential([\n",
    "                layers.Input(shape=(X_train.shape[1], X_train.shape[2])),  # (seq_len, embedding_dim)\n",
    "\n",
    "                layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "                layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "                layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "                layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "                layers.GlobalMaxPooling1D(),\n",
    "\n",
    "                layers.Dense(128, activation='relu'),\n",
    "                layers.Dropout(0.3),\n",
    "\n",
    "                layers.Dense(Y_all_encoded.shape[1], activation='softmax')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=sgd,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # ================================\n",
    "            # Medição de tempo de treinamento\n",
    "            # ================================\n",
    "            start_fit = time.time()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                epochs=n_epocas,\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history.history)\n",
    "            \n",
    "            end_fit = time.time()\n",
    "            fit_duration = end_fit - start_fit\n",
    "            cv_results['fit_time'].append(fit_duration)\n",
    "\n",
    "            # ================================\n",
    "            # Medição de tempo de predição\n",
    "            # ================================\n",
    "            start_score = time.time()\n",
    "            y_pred_test = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "            y_pred_train = np.argmax(model.predict(X_train_all, verbose=0), axis=1)\n",
    "            end_score = time.time()\n",
    "            score_duration = end_score - start_score\n",
    "            cv_results['score_time'].append(score_duration)\n",
    "\n",
    "            # Decodificar labels para índices\n",
    "            y_true_test = np.argmax(y_test, axis=1)\n",
    "            y_true_train = np.argmax(y_train_all, axis=1)\n",
    "\n",
    "            # ================================\n",
    "            # Cálculo de métricas\n",
    "            # ================================\n",
    "            cv_results['test_accuracy'].append(accuracy_score(y_true_test, y_pred_test))\n",
    "            cv_results['train_accuracy'].append(accuracy_score(y_true_train, y_pred_train))\n",
    "\n",
    "            cv_results['test_precision'].append(precision_score(y_true_test, y_pred_test, average='macro', zero_division=0))\n",
    "            cv_results['train_precision'].append(precision_score(y_true_train, y_pred_train, average='macro', zero_division=0))\n",
    "\n",
    "            cv_results['test_recall'].append(recall_score(y_true_test, y_pred_test, average='macro', zero_division=0))\n",
    "            cv_results['train_recall'].append(recall_score(y_true_train, y_pred_train, average='macro', zero_division=0))\n",
    "\n",
    "            cv_results['test_f1_macro'].append(f1_score(y_true_test, y_pred_test, average='macro', zero_division=0))\n",
    "            cv_results['train_f1_macro'].append(f1_score(y_true_train, y_pred_train, average='macro', zero_division=0))\n",
    "\n",
    "        cvr = pd.DataFrame(cv_results)\n",
    "        cvr[\"company\"] = c\n",
    "        cvr[\"model_name\"] = m\n",
    "        cvr[\"vec_name\"] = v\n",
    "\n",
    "        model_metrics.append(cvr)\n",
    "\n",
    "        # =============================\n",
    "        # Organização das métricas\n",
    "        # =============================\n",
    "        # Concatena todas as métricas\n",
    "        all_metrics = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "\n",
    "        for h in histories:\n",
    "            for k in all_metrics.keys():\n",
    "                all_metrics[k].extend(h[k])\n",
    "\n",
    "        # Cria DataFrame\n",
    "        metrica_labels = ['Train loss'] * n_epocas * repeticao_model + \\\n",
    "                         ['Val loss'] * n_epocas * repeticao_model + \\\n",
    "                         ['Train Acc'] * n_epocas * repeticao_model + \\\n",
    "                         ['Val Acc'] * n_epocas * repeticao_model\n",
    "\n",
    "        epocas = np.tile(np.arange(1, n_epocas + 1), 4 * repeticao_model)\n",
    "        dados = all_metrics['loss'] + all_metrics['val_loss'] + all_metrics['accuracy'] + all_metrics['val_accuracy']\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Epochs': epocas,\n",
    "            'Metrics': metrica_labels,\n",
    "            'Variation': dados\n",
    "        })\n",
    "\n",
    "        # =============================\n",
    "        # Plot\n",
    "        # =============================\n",
    "        save_file = f\"/plot_results/cnn/{c}_{v}.png\".lower()\n",
    "        create_plot_cnn(df, save_file, n_epocas)\n",
    "\n",
    "    model_metrics_all += model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_metrics_all = pd.concat(model_metrics_all)\n",
    "df_model_metrics_all.to_excel(\"reclamacoes_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
